# ch02

01.学习了 Input Embedding 的总体流程，从将 input 转化为 tokens，再转化为 tokeIDs，再进行词嵌入和位置信息      的嵌入，共同构成的输入信息的嵌入表示(主要代码，后面多数代码重复的)

02.比较了不同BPE实现策略的效率（未关注具体的实现策略）

03.比较了线性层和嵌入层的关系，二者的本质是一样的

![1731143538217](image/Record/1731143538217.png)
![1731143519808](image/Record/1731143519808.png)

04.直接对token_ids进行操作，加深对dataloader的理解

# ch03

01.这个章节主要介绍了自注意力机制的实现，将input embedding转化为了context向量，即考虑了上下文；然后又介绍了利用Query、Key、Value三个矩阵实现一个可训练的自注意力层（下图）；然后考虑到隐藏未来的信息，又介绍了一个因果注意力，对未来部分进行一个掩码操作；最后讲了一个通过堆叠单头注意力层实现的多头注意力层，这里部分代码有点没看懂

![1731767252910](image/Record/1731767252910.png)

# ch04

01.

图片是一个Transform Block的内容

![1732100241900](image/Record/1732100241900.png)

![1732102697152](image/Record/1732102697152.png)


![1732104120810](image/Record/1732104120810.png)




上下文向量处还要看一下，可能是转化为context vector时对应的向量就是由前面的值预测的结果可能性了

以及ch04的最后可以再看一下
